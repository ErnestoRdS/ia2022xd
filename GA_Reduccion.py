# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1icyiOglHfjlU6CKWeWP278XLKJG-NfXw
"""

from sklearn.neighbors import KNeighborsClassifier  # clasificador a usar
from sklearn.metrics import accuracy_score  # calcula la exactitud
from sklearn.model_selection import (
    train_test_split,
)  # permite separar el dataset en prueba y entrenamiento
from sklearn.datasets import load_digits  # carga el dataset
import numpy as np  # para manipular arreglos

X, y = load_digits(return_X_y=True)  # carga dataset separado por vectores y etiquetas

dim = X.shape[1]  # obtiene la dimensionalidad del problema

print(f"Dimension original: {dim}")

Xtr, Xte, ytr, yte = train_test_split(
    X, y, test_size=0.5, stratify=y
)  # separa el conjunto de datos en 90% entrenamiento y 10% prueba

candidate_solution = np.random.randint(
    low=0, high=2, size=dim
)  # genera un vector de 0s y 1s aleatoriamente, esto lo debe proponer el algoritmo metaheuristico; es la solucion candidata
print(candidate_solution.shape)

print(
    f"Dimension propuesta: {np.sum(candidate_solution)}"
)  # reportamos la cantidad de caracteristicas seleccionadas
print(
    f"Indices seleccionados: {np.where(candidate_solution==1)}"
)  # reportamos los indices seleccionados (valores 1)

reduced_Xtr = Xtr[
    :, candidate_solution == 1
]  # filtramos las caracteristicas seleccionadas del conjunto de entrenamiento
reduced_Xte = Xte[
    :, candidate_solution == 1
]  # filtramos las caracteristicas seleccionadas del conjunto de prueba

# Generamos un clasificador con el conjunto de datos completo, esto no se requiere, es para ilustracion del comportamiento con toda la informacion
full_Knn = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)
full_Knn.fit(Xtr, ytr)
# Generamos y entrenamos un clasificador con el conjunto de datos con reduccion de dimensionalidad, esto si iria en el algoritmo metaheuristico
reduced_Knn = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)
reduced_Knn.fit(reduced_Xtr, ytr)

# Obtenemos la exactitud del conjunto de entrenamiento con reduccion de caracteristicas
reduced_acc_tr = accuracy_score(ytr, reduced_Knn.predict(reduced_Xtr))

# Reportamos la exactitud de ambos clasificadores para el conjunto de entrenamiento -no es necesario trabajar el conjunto de datos completos, solo es con fines ilustrativos-
print(
    f"[Full-KNN] Exactitud Entrenamiento: {accuracy_score(ytr, full_Knn.predict(Xtr))}"
)
print(f"[Reduced-KNN] Exactitud Entrenamiento: {reduced_acc_tr}")

# Calculamos la funcion objetivo con el conjunto de entranmiento
fitness = 0.8 * (1 - reduced_acc_tr) + 0.2 * (np.sum(candidate_solution) / dim)

# Reportamos la aptitud de la propuesta con fines ilustrativos
print(f"Aptitud de la solucion candidata {fitness}")

# Reportamos la exactitud de ambos clasificadores para el conjunto de prueba -no es necesario trabajar el conjunto de datos completos, solo es con fines ilustrativos-
print(f"[Full-KNN] Exactitud Prueba: {accuracy_score(yte, full_Knn.predict(Xte))}")
print(
    f"[Reduced-KNN] Exactitud Prueba: {accuracy_score(yte, reduced_Knn.predict(reduced_Xte))}"
)
